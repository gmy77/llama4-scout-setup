# ğŸš€ Llama 4 Scout - Setup Completo

## ğŸ“¦ File Inclusi

```
ğŸ“ Progetto Llama 4 Scout
â”œâ”€â”€ ğŸ“„ README.md                    â† Sei qui!
â”œâ”€â”€ ğŸ“„ GUIDA_LLAMA4_SCOUT.md       â† Guida completa dettagliata
â”œâ”€â”€ ğŸ”§ install_llama4.sh           â† Script installazione dipendenze
â”œâ”€â”€ ğŸ llama4_scout_setup.py       â† Script completo con 4 modalitÃ 
â””â”€â”€ ğŸ test_veloce.py              â† Test rapido per verificare funzionamento
```

---

## âš¡ Quick Start (3 passi)

### 1ï¸âƒ£ Installa le dipendenze
```bash
chmod +x install_llama4.sh
./install_llama4.sh
```

### 2ï¸âƒ£ Configura Hugging Face
```bash
# Login (ti verrÃ  chiesto il token)
huggingface-cli login

# Poi vai su questa pagina e accetta la licenza:
# https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct
```

### 3ï¸âƒ£ Esegui il test
```bash
python3 test_veloce.py
```

âœ… Se tutto funziona, sei pronto!

---

## ğŸ“š ModalitÃ  d'Uso

### ğŸ¯ ModalitÃ  1: Test Veloce (Consigliato per iniziare)
```bash
python3 test_veloce.py
```
- Carica il modello automaticamente
- Esegue 3 test di esempio
- Perfetto per verificare che tutto funzioni

### ğŸ¯ ModalitÃ  2: Script Completo
```bash
python3 llama4_scout_setup.py
```
Poi scegli:
- **Opzione 1:** Pipeline semplice (solo testo)
- **Opzione 2:** AutoModel avanzato (piÃ¹ controllo)
- **Opzione 3:** Multimodale (con immagini!)
- **Opzione 4:** Chat interattivo

### ğŸ¯ ModalitÃ  3: Chat Interattivo
```bash
python3 llama4_scout_setup.py
# Scegli opzione 4
```
Chatta con il modello in tempo reale!

---

## ğŸ–¥ï¸ Requisiti Minimi

### Hardware
- **RAM:** 32 GB (64 GB consigliato)
- **GPU:** NVIDIA con 24 GB VRAM
  - âœ… RTX 4090
  - âœ… A100 (40GB/80GB)
  - âœ… RTX 6000 Ada
  - âš ï¸ RTX 3090 (al limite)

### Software
- **Python:** 3.8+
- **CUDA:** 11.8 o 12.1+
- **Account Hugging Face** (gratuito)

---

## ğŸ”‘ Setup Hugging Face (Dettagliato)

### Passo 1: Crea account
1. Vai su https://huggingface.co
2. Registrati gratuitamente

### Passo 2: Genera token
1. Vai su https://huggingface.co/settings/tokens
2. Clicca "New token"
3. Nome: `llama4-access`
4. Tipo: **Read**
5. Copia il token

### Passo 3: Login
```bash
pip install huggingface-hub
huggingface-cli login
# Incolla il token quando richiesto
```

### Passo 4: Accetta licenza
1. Vai su https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct
2. Clicca "Agree and access repository"
3. Attendi approvazione (da 5 minuti a 2 ore)

---

## ğŸ“ Esempi di Codice

### Esempio Base
```python
from transformers import pipeline
import torch

pipe = pipeline(
    "text-generation",
    model="meta-llama/Llama-4-Scout-17B-16E-Instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16
)

messages = [{"role": "user", "content": "Ciao!"}]
output = pipe(messages, max_new_tokens=200)
print(output[0]["generated_text"][-1]["content"])
```

### Con Immagine
```python
from transformers import AutoProcessor, Llama4ForConditionalGeneration

processor = AutoProcessor.from_pretrained(
    "meta-llama/Llama-4-Scout-17B-16E-Instruct"
)
model = Llama4ForConditionalGeneration.from_pretrained(
    "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16
)

messages = [{
    "role": "user",
    "content": [
        {"type": "image", "url": "https://example.com/foto.jpg"},
        {"type": "text", "text": "Cosa vedi?"}
    ]
}]

inputs = processor.apply_chat_template(
    messages, 
    tokenize=True, 
    return_tensors="pt"
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=200)
```

---

## ğŸ”§ Risoluzione Problemi Comuni

### âŒ "OutOfMemoryError"
**Problema:** GPU non ha abbastanza memoria

**Soluzione 1:** Usa quantizzazione 8-bit
```python
model = Llama4ForConditionalGeneration.from_pretrained(
    MODEL_ID,
    load_in_8bit=True,
    device_map="auto"
)
```

**Soluzione 2:** Usa quantizzazione 4-bit (ancora piÃ¹ leggera)
```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4"
)

model = Llama4ForConditionalGeneration.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto"
)
```

### âŒ "Repository not found" o "401 Unauthorized"
**Problema:** Non hai accesso al modello

**Soluzione:**
1. Verifica di aver accettato la licenza su Hugging Face
2. Riprova il login: `huggingface-cli login`
3. Attendi l'approvazione (puÃ² richiedere tempo)

### âŒ Modello molto lento
**Problema:** Sta usando CPU

**Soluzione:**
```python
# Verifica GPU
import torch
print(torch.cuda.is_available())  # Deve essere True

# Forza GPU
pipe = pipeline(
    "text-generation",
    model=MODEL_ID,
    device=0,  # Usa GPU 0
    torch_dtype=torch.bfloat16
)
```

### âŒ "Some weights not initialized"
**Non Ã¨ un errore!** Ãˆ normale per modelli grandi. Ignora.

---

## ğŸ“– Documentazione Completa

Per la guida dettagliata con tutte le funzionalitÃ  avanzate:
```bash
cat GUIDA_LLAMA4_SCOUT.md
# oppure aprila con un editor di testo
```

La guida include:
- Quantizzazione avanzata
- Streaming delle risposte
- Ottimizzazione memoria
- Tuning dei parametri
- Esempi multimodali completi
- Best practices per prompt
- Benchmark e performance

---

## ğŸ¯ Caratteristiche Llama 4 Scout

### âœ¨ Punti di Forza
- **Multimodale:** Testo + Immagini nativo
- **12 Lingue:** Include italiano perfettamente
- **17B parametri attivi** su 109B totali (MoE)
- **Context:** Fino a 128K token
- **Efficiente:** Gira su GPU singola
- **Veloce:** ~20-30 token/secondo

### ğŸ“Š Performance
- **First token:** 2-3 secondi
- **Throughput:** 20-30 token/s
- **QualitÃ :** Eccellente in italiano
- **Comprensione immagini:** Stato dell'arte

---

## ğŸ†š Alternative

Se hai problemi con Llama 4 Scout, considera:

| Modello | Parametri | VRAM | VelocitÃ  | Italiano |
|---------|-----------|------|----------|----------|
| **Llama 4 Scout** | 17B (109B) | 24GB | âš¡âš¡âš¡ | â­â­â­â­â­ |
| Llama 3.3 70B | 70B | 48GB+ | âš¡âš¡ | â­â­â­â­â­ |
| Llama 3 8B | 8B | 16GB | âš¡âš¡âš¡âš¡ | â­â­â­â­ |
| Mistral 7B | 7B | 14GB | âš¡âš¡âš¡âš¡ | â­â­â­â­ |

---

## ğŸ’¡ Tips Utili

### Per Risparmiare Memoria
```python
# Libera cache tra generazioni
torch.cuda.empty_cache()
```

### Per Risposte Migliori
```python
# PiÃ¹ deterministico
output = pipe(messages, temperature=0.3)

# PiÃ¹ creativo
output = pipe(messages, temperature=0.9)

# Bilanciato
output = pipe(messages, temperature=0.7, top_p=0.9)
```

### Per Conversazioni Lunghe
```python
# Usa system prompt
messages = [
    {"role": "system", "content": "Sei un assistente esperto."},
    {"role": "user", "content": "Domanda..."}
]
```

---

## ğŸ¤ Supporto

### Problemi con questo setup?
1. Controlla [Risoluzione Problemi](#-risoluzione-problemi-comuni)
2. Leggi `GUIDA_LLAMA4_SCOUT.md`
3. Verifica i log di errore

### Problemi con il modello?
- [Hugging Face Forum](https://discuss.huggingface.co)
- [Discord Hugging Face](https://hf.co/join/discord)
- [Model Card](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct)

---

## ğŸ“ˆ Roadmap

Prossimi miglioramenti a questi script:
- [ ] Supporto per quantizzazione GPTQ/AWQ
- [ ] Esempio con RAG (Retrieval Augmented Generation)
- [ ] Fine-tuning guidato
- [ ] Deployment con FastAPI
- [ ] Integrazione con LangChain

---

## â­ Feedback

Questo setup ti Ã¨ stato utile? Hai suggerimenti?
Fammi sapere come posso migliorarlo!

---

## ğŸ“„ Licenza

Questi script sono forniti "as-is" per uso personale.

Il modello Llama 4 Scout Ã¨ soggetto alla licenza Llama 4 Community License Agreement di Meta.
Leggi i termini su: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct

---

**ğŸ‰ Buon divertimento con Llama 4 Scout!**
